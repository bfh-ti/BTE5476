{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div style=\"margin: 0 auto 10px; height: 70px; border: 2px solid gray; border-radius: 6px;\">\n",
    "  <div style=\"float: left; margin: 5px 10px 5px 10px; \"><img src=\"img/bfh.jpg\" /></div>\n",
    "  <div style=\"float: right; margin: 20px 30px 0; font-size: 15pt; font-weight: bold; color: #98b7d2;\"><a href=\"https://moodle.bfh.ch/course/view.php?id=39255\" style=\"color: #98b7d2;\">BTE5476 - Project-Oriented Digital Signal Processing </a></div>\n",
    "</div>\n",
    "<div style=\"clear: both; font-size: 30pt; font-weight: bold; color: #64788b; margin-left: 30px;\">\n",
    "    Autotune\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.signal as sp\n",
    "from IPython.display import Audio\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 14, 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiplay(clips, sf, title=None):\n",
    "    outs = [widgets.Output() for c in clips]\n",
    "    for ix, item in enumerate(clips):\n",
    "        with outs[ix]:\n",
    "            print(title[ix] if title is not None else \"\")\n",
    "            display(Audio(item, rate=sf, normalize=True))\n",
    "    return widgets.HBox(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pre-load audio samples in a dictionary\n",
    "audio, audio_sf = {}, None\n",
    "for file in ['speech', 'music', 'yesterday', 'voiced', \n",
    "            'sing-whisper', 'sing-robot', 'sing-daft', \n",
    "            'talk-whisper', 'talk-robot', 'talk-daft'] :\n",
    "    sf, x = wavfile.read(f'data/{file}.wav')\n",
    "    if audio_sf is None:\n",
    "        audio_sf = sf\n",
    "    else:\n",
    "        assert sf == audio_sf, 'notebook requires sampling rates for all audio samples to be the same'\n",
    "    audio[file] = (x - np.mean(x)) / 32767.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_spec(x, Fs, max_freq=None, do_fft=True):\n",
    "    C = int(len(x) / 2)  # positive frequencies only\n",
    "    if max_freq:\n",
    "        C = int(C * max_freq / float(Fs) * 2) \n",
    "    X = np.abs(np.fft.fft(x)[0:C]) if do_fft else x[0:C]\n",
    "    N = Fs * np.arange(0, C) / len(x);\n",
    "    plt.plot(N, X)\n",
    "    return N, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ms2n(ms, sf):\n",
    "    return int(float(sf) * ms / 1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# frequency ratio between successive semitones in 12-tone equal temperament\n",
    "semitone = 2 ** (1.0 / 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Auto-Tune\n",
    "\n",
    "<img width=\"300\" style=\"float: right; margin: 0 30px 0 30px;\" src=\"img/autotune.jpg\"> \n",
    "\n",
    "[Auto-Tune](https://en.wikipedia.org/wiki/Auto-Tune) is a commercial pitch correction algorithm designed specifically for vocal audio tracks; by combining pitch estimation with pitch shifting, an autotune plug-in can remove any slightly out-of-tune notes in a singer's performance. In this notebook we will study how to adapt a standard pitch-shifting algorithm to the specificities of the human voice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Pitch shifting: recap\n",
    "\n",
    "The pitch shifting methods we have implemented in the previous notebooks used time scaling followed by resampling; more precisely, to shift the pitch of an audio signal by a factor $\\beta$:\n",
    " * we time-scale the signal by a factor $1/\\beta$ using a phase vocoder (i.e. increase its length if raising the pitch and vice-versa)\n",
    " * we resample the time-scaled signal by a factor $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## The building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hann_cola(N: int, K=2) -> (np.ndarray, int, int):\n",
    "    if K <= 1:\n",
    "        return np.ones(N), N, N\n",
    "    S = N // K\n",
    "    N = K * S + 1\n",
    "    win = (2 / K) * np.sin(np.pi * np.arange(0, N) / (N-1)) ** 2\n",
    "    return win, N, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resample(x, alpha):\n",
    "    N = len(x)\n",
    "    y, m = np.zeros(int(N / alpha + 0.5)), 0\n",
    "    while True:\n",
    "        t = m * alpha \n",
    "        n = int(np.floor(t))\n",
    "        if n < N - 1:\n",
    "            y[m] = (n + 1 - t) * x[n] + (t - n) * x[n + 1] \n",
    "            m += 1\n",
    "        else:\n",
    "            break\n",
    "    return y[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def timescale_pv(x, alpha, grain_size, K=4):\n",
    "    win, N, S = hann_cola(grain_size, K)\n",
    "    a_hop, s_hop = int(S * alpha), S \n",
    "    \n",
    "    wk = 2 * np.pi * np.arange(0, N) / N\n",
    "    phase = prev_phase = np.angle(np.fft.fft(x[:N]))\n",
    "\n",
    "    def wrap_angle(w):\n",
    "        return w - 2 * np.pi * np.round(w / 2 / np.pi)\n",
    "    \n",
    "    y = np.zeros(int(len(x) / alpha + 1))    \n",
    "    n, m = 0, 0\n",
    "    while n < len(x) - N and m < len(y) - N:\n",
    "        grain_fft = np.fft.fft(win * x[n:n + N])\n",
    "        grain_phase = np.angle(grain_fft)\n",
    "        phase_delta = wrap_angle(grain_phase - prev_phase - a_hop * wk)\n",
    "        prev_phase = grain_phase\n",
    "        phase = wrap_angle(phase + s_hop * (wk + phase_delta / a_hop))\n",
    "        grain = np.real(np.fft.ifft(np.abs(grain_fft) * np.exp(1j * phase)))\n",
    "        y[m:m+N] += grain * win\n",
    "        n += a_hop\n",
    "        m += s_hop\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Pitch shifting (two passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pitchshift_pv(x, beta, grain_size, K=3):\n",
    "    return(resample(timescale_pv(x, 1 / beta, grain_size, K), beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shift = 3\n",
    "\n",
    "for file, gs_ms in [('speech', 80), ('music', 200)]:\n",
    "    gs = ms2n(gs_ms, audio_sf)\n",
    "    display(multiplay(\n",
    "        [ audio[file], *( pitchshift_pv(audio[file], semitone ** s, gs) for s in [shift, -shift] ) ], \n",
    "        audio_sf, \n",
    "        [f'{file} sample', 'higher pitch', 'lower pitch' ]\n",
    "    ))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Warm-up exercise (optional): single-pass implementation\n",
    "\n",
    "Implement the pitch shifting algorithm in causal form (i.e. using a single pass). Here is a possible approach (and note how the analysis and sythesis hops are both equal to the stride):\n",
    "\n",
    " * choose the synthesis grain length $N$; the analysis grain size will be $M=\\lfloor \\beta N \\rfloor$\n",
    " * choose the grain overlap factor $K$ and obtain the COLA synthesis stride $S$ \n",
    " * iterate over $k = 0, 1, \\ldots$:\n",
    "     * extract a grain of length $M$ from the input at time $n = kS$\n",
    "     * resample the grain with factor $\\beta$ to obtain a grain of length $N$\n",
    "     * compute the phase offset wrt the previous output grain and update the global phase\n",
    "     * set the new grain's phase to the global phase and obtain  $g'_k[n]$\n",
    "     * add $w[n-kS]g'_k[n-kS]$ to the output signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pitchshift_pv_rt(x, beta, grain_size, K=3):\n",
    "    win, N, S = hann_cola(grain_size, K)\n",
    "    y = np.zeros_like(x)   \n",
    "    \n",
    "    # your code here...\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file, gs_ms in [('speech', 120), ('music', 200)]:\n",
    "    gs = ms2n(gs_ms, audio_sf)\n",
    "    display(multiplay(\n",
    "        [ audio[file], *( pitchshift_pv_rt(audio[file], semitone ** s, gs) for s in [shift, -shift] ) ], \n",
    "        audio_sf, \n",
    "        [f'{file} sample', 'higher pitch', 'lower pitch' ]\n",
    "    ))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Pitch-shifting vocals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "When we pitch-shift a singing voice via time scaling and resampling, the result doesn't sound very natural:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file, gs_ms = 'yesterday', 160\n",
    "\n",
    "shift = 2\n",
    "\n",
    "display(multiplay(\n",
    "    [ audio[file], *( pitchshift_pv(audio[file], semitone ** s, ms2n(gs_ms, audio_sf)) for s in [shift, -shift] ) ], \n",
    "    audio_sf, \n",
    "    [f'{file} sample', 'higher pitch', 'lower pitch' ]\n",
    "))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The reason for this is due to the particular structure of a voice signal, which needs to be taken into account by the pitch shifting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Predictive Coding of speech signals\n",
    "\n",
    "LPC is a speech analysis technique widely used to compress voice signal to very low bitrates. It is based on a source-filter model for speech production and on the estimation of the time-varying model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The source-filter model for voice\n",
    "\n",
    "<img width=\"400\" style=\"float: right; margin-right: 30px;\" src=\"img/lpc.jpg\"> \n",
    "\n",
    "Vocal production can be modeled as an *excitation source signal* shaped by the frequency response of the *vocal tract*; the peaks of the frequency response are called the *formants* and their position changes according to the position of moving parts such as lips, tongue, jaw, etc. \n",
    "\n",
    "The source is time-varying and produces either:\n",
    " * a periodic signal generated by the vibration of the vocal cords (voiced sounds)\n",
    " * a noise-like signal generated by an air flow (unvoiced sounds)\n",
    " \n",
    "The filter's response depends on:\n",
    " * the shape of the vocal tract (larynx, mouth, nose: time varying)\n",
    " * the resonances in head and chest (fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The spectrum of a short voiced speech segment shows the harmonic nature of the sound and the overall envelope determined by the head and mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the spectrum of a short voiced speech segment and the estimated frequency response of the vocal tract\n",
    "#  (the actual response will be computed explicitly later)\n",
    "vs_env = np.fft.fft([1.0, -2.1793, 2.4140, -1.6790, 0.3626, 0.5618, -0.7047, \n",
    "                0.1956, 0.1872, -0.2878, 0.2354, -0.0577, -0.0815, 0.0946, \n",
    "                0.1242, -0.1360, 0.0677, -0.0622, -0.0306, 0.0430, -0.0169], len(audio['voiced']));\n",
    "plot_spec(audio['voiced'], audio_sf);\n",
    "plot_spec(np.abs(np.divide(1.0, vs_env)), audio_sf, do_fft=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Key observations:\n",
    "\n",
    " * the frequency response of the vocal tract is independent of the source excitation\n",
    " * to pitch-shift speech we should only modify the excitation\n",
    " * if we shift the whole signal, the envelope of the vocal tract is shifted as well and the voice sounds unnatural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## LPC analysis\n",
    "\n",
    "The source-filter model for voice production is:\n",
    "\n",
    "$$\n",
    "    X(z) = A(z)E(z)\n",
    "$$\n",
    "\n",
    "where\n",
    " * $A(z)$: frequency response of the vocal tract for the segment\n",
    " * $E(z)$: excitation signal (either noise or periodic oscillation)\n",
    "\n",
    "Both are unkonwn and we need to find _both_ from the samples $x[n]$. Under some reasonable assumptions, $A(z)$ can be determined by computing the so-called Linear Prediction Coefficients (LPC) of a speech segment; the length of the analysis windows must be short enough so that the nature of the source and the frequency response of the filter do not change within each segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### The all-pole filter model\n",
    "\n",
    "We model the vocal tract as an all-pole filter of order $p$; this is reasonable since the vocal tract is a passive filter with clear resonance peaks:\n",
    "\n",
    "$$\n",
    "  A(z) = \\frac{1}{1 - \\sum_{k=1}^{p}a_kz^{-k}}\n",
    "$$ \n",
    "\n",
    "In the time domain we have\n",
    "$$\n",
    "  x[n] = \\sum_{k=1}^{p}a_k x[n-k] + e[n]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### The AR estimation problem\n",
    "\n",
    "In the absence of an external excitation, the all-pole model states that $x[n]$ is fully determined by $x[n-1], \\dots, x[n-p]$; we call this an *autoregressive* (AR) signal. \n",
    "\n",
    "With an external excitation, if we knew the $a_k$ we could exactly predict $x[n]$ and the \"error\" would be the excitation itself, which is supposed to be independent of $x[n]$\n",
    "\n",
    "$$\n",
    "  e[n] = x[n] - \\sum_{k=1}^{p}a_k x[n-k]\n",
    "$$\n",
    "\n",
    "The optimal set of $a_k$ minimizes the energy of the unpredictable error component, $E[e^2[n]]$:\n",
    " * if $E[e^2[n]]$ is minimized, then $e[n]$ is orthogonal to $x[n]$ (see the [orhtogonality principle](https://en.wikipedia.org/wiki/Orthogonality_principle))\n",
    " * orthogonal signals have no shared information: we have separated excitation and source!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### The linear prediction coefficients\n",
    "\n",
    "The coefficients of the filter $A(z)$ can be found as\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        r_0 & r_1 & r_2 & \\ldots & r_{p-1} \\\\\n",
    "        r_1 & r_0 & r_1 & \\ldots & r_{p-2} \\\\        \n",
    "        & & & \\vdots \\\\\n",
    "        r_{p-1} & r_{p-2} & r_{p-3} & \\ldots & r_{0} \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        a_1 \\\\\n",
    "        a_2 \\\\        \n",
    "        \\vdots \\\\\n",
    "        a_{p}\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        r_1 \\\\\n",
    "        r_2 \\\\        \n",
    "        \\vdots \\\\\n",
    "        r_{p}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $r$ is the biased autocorrelation of the $N$-point input data:\n",
    "\n",
    "$$\n",
    "  r_m = (1/N)\\sum_{k = 0}^{N-m-1}x[k]x[k+m]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Because of the Toeplitz structure of the autocorrelation matrix, the system of equations can be solved very efficiently using the Levinson-Durbin algorithm. Here is a direct implementation of the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autocorrelation(x: np.ndarray, p: int) -> np.ndarray:\n",
    "    # compute the biased autocorrelation for x up to lag p\n",
    "    N = len(x)\n",
    "    assert N > p, f'vector x must contain at least {p} elements'\n",
    "    r = np.zeros(p+1)\n",
    "    for m in range(0, p+1):\n",
    "        for n in range(0, N-m):\n",
    "            r[m] += x[n] * x[n+m]\n",
    "        r[m] /= float(N)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def levinson_durbin(r: np.ndarray, p: int) -> np.ndarray:\n",
    "    assert len(r) > p, f'vector r must have at least {p} elements'\n",
    "    g = r[1] / r[0]\n",
    "    a = np.array([g])\n",
    "    v = (1. - g * g) * r[0];\n",
    "    for i in range(1, p):\n",
    "        g = (r[i+1] - np.dot(a, r[1:i+1])) / v\n",
    "        a = np.r_[ g,  a - g * a[i-1::-1] ]\n",
    "        v *= 1. - g * g\n",
    "    # return the coefficients of the A(z) filter\n",
    "    return np.r_[1, -a[::-1]]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lpc(x: np.ndarray, p: int) -> np.ndarray:\n",
    "    # compute p LPC coefficients for a speech segment\n",
    "    return levinson_durbin(autocorrelation(x, p), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "let's plot the previous figure again by direct computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = audio['voiced']\n",
    "a = lpc(x, 20)\n",
    "\n",
    "A = np.abs(np.divide(1.0, np.fft.fft(a, len(audio['voiced']))))  \n",
    "\n",
    "plot_spec(x, audio_sf);\n",
    "plot_spec(A, audio_sf, do_fft=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exc = sp.lfilter(a, [1], x)\n",
    "plt.plot(x)\n",
    "plt.plot(exc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: LPC speech compression\n",
    "\n",
    "Basic encoding:\n",
    "    * segment speech in 20ms chunks\n",
    "    * find $A(z)$ using LPC analysis\n",
    "    * inverse filter to obtain the excitation signal \n",
    "    * determine if $e[n]$ is periodic (voiced) and if so find the period\n",
    "    * data per chunk: $P$ LPC coefficients + source period\n",
    "\n",
    "Typical compression rate: \n",
    " * PCM speech signal sampled at 8kHz, 8 bits/sample: 48 kbs\n",
    " * LPC-coded, 40ms frames, 20 LPC coefficients: 4 kbs\n",
    "\n",
    "Mobile phones (GSM) use LPC-based encoding for voice transmission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Vocal pitch shifting using LPC\n",
    "\n",
    "Once we have separated the source from the filter, vocal pitch shifting can be performed by pitch shifting only the excitation; here is a single-pass implementation using the phase vocoder.\n",
    "\n",
    "For a pitcshift factor $\\beta$: \n",
    "\n",
    " * choose a *synthesis* grain size $N$\n",
    " * choose the grain overlap factor $K$ and obtain the COLA synthesis stride $S$ \n",
    " * the _analysis_ grain size will be $M = \\lfloor S \\beta \\rfloor$ \n",
    " * iterate over $k = 0, 1, \\ldots$:\n",
    "     * extract a grain of length $M$ from the input at time $n=kS$\n",
    "     * compute the LPC coefficients for the grain\n",
    "     * inverse-filter the grain to recover the excitation signal\n",
    "     * resample the excitation with a factor $\\beta$\n",
    "     * forward-filter the resampled excitation to apply the original vocal tract envelope\n",
    "     * adjust the phase of the grain with the phase vocoder\n",
    "     * add the resynthesized grain to the output signal at $n=kS$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pitchshift_lpc(x, beta, grain_size, K=3, order=20):\n",
    "    win, N, S = hann_cola(grain_size, K)\n",
    "    M = int((N + order) * beta)\n",
    "    \n",
    "    wk = 2 * np.pi * np.arange(0, N) / N\n",
    "    phase = prev_phase = np.angle(win * np.fft.fft(x[:N]))\n",
    "\n",
    "    filter_state = np.zeros(order)\n",
    "\n",
    "    def wrap_angle(w):\n",
    "        return w - 2 * np.pi * np.round(w / 2 / np.pi)\n",
    "    \n",
    "    y = np.zeros_like(x)    \n",
    "    for n in range(0, len(x) - max(N, M), S):\n",
    "        chunk = x[n:n+M]\n",
    "        a = lpc(chunk, order)\n",
    "        exc = sp.lfilter(a, [1], chunk)\n",
    "        exc = resample(exc, beta)\n",
    "        grain, filter_state = sp.lfilter([1], a, exc, zi=filter_state)\n",
    "\n",
    "        grain_fft = np.fft.fft(win * grain[:N])\n",
    "        grain_phase = np.angle(grain_fft)\n",
    "        phase_delta = wrap_angle(grain_phase - prev_phase - S * wk)\n",
    "        prev_phase = grain_phase\n",
    "        phase = wrap_angle(phase + S * wk + phase_delta)\n",
    "        grain = np.real(np.fft.ifft(np.abs(grain_fft) * np.exp(1j * phase)))\n",
    "        y[n:n+N] += grain * win\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file, gs_ms = 'yesterday', 160\n",
    "\n",
    "shift = 2\n",
    "\n",
    "for algo in [pitchshift_pv, pitchshift_lpc]:\n",
    "    display(multiplay(\n",
    "        [ audio[file], *( algo(audio[file], semitone ** s, ms2n(gs_ms, audio_sf), K=3) for s in [shift, -shift] ) ], \n",
    "        audio_sf, \n",
    "        ['vocal sample', f'higher pitch ({algo.__name__})', f'lower pitch ({algo.__name__})' ]\n",
    "    ))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Fun with the vocoder\n",
    "\n",
    "The digital phase vocoder that we implemented earlier is a modern variation of the original analog [vocoder](https://en.wikipedia.org/wiki/Vocoder), the first speech compression device. The vocoder was developed in the 1930s in order to increase the efficiency of existing telecommunication channels and it was later used to build the first voice encryption device used by the allies during WWII.\n",
    "\n",
    "The vocoder worked by performing a source-filter decomposition of the speech signal, much in the same way as what we achieved via LPC analysis. When the vocoder's output was resynthesized into a speech signal, however, it was quickly noticed that one could create very interesting and unexpected sounds by replacing the original source signal with a synthetic one. The vocoder quickly attracted the attention of artists and musicians and, to this day, its modern digital implementation is used to create a variety of sound effects.\n",
    "\n",
    "As an example, here are three classic voice effects that can be achieved applying a modified vocoder to our usual audio samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "modes = ['whisper', 'robot', 'daft']\n",
    "for voice in ['talk', 'sing']:\n",
    "    display(widgets.VBox([widgets.Label(voice), multiplay([ audio[f'{voice}-{mode}'] for mode in modes ], audio_sf, modes) ]  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Modify the LPC-based autotune algorithm to reproduce as well as you can the three vocoder \"styles\" demonstrated above. Feel free to get creative and experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vocoder(x, mode, sf, **argv):\n",
    "    # mode selects among 'whisper', 'robot', 'daft'\n",
    "    \n",
    "    y = np.zeros_like(x)    \n",
    "\n",
    "    # your code here...\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "modes = ['whisper', 'robot', 'daft']\n",
    "for file in ['speech', 'yesterday']:\n",
    "    display(multiplay(\n",
    "        [ audio[file], *( vocoder(audio[file], mode, audio_sf) for mode in modes ) ], \n",
    "        audio_sf, \n",
    "        [f'{file} (original)',  *modes]\n",
    "    ))    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
