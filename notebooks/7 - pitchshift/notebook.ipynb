{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div style=\"margin: 0 auto 10px; height: 70px; border: 2px solid gray; border-radius: 6px;\">\n",
    "  <div style=\"float: left; margin: 5px 10px 5px 10px; \"><img src=\"img/bfh.jpg\" /></div>\n",
    "  <div style=\"float: right; margin: 20px 30px 0; font-size: 15pt; font-weight: bold; color: #98b7d2;\"><a href=\"https://moodle.bfh.ch/course/view.php?id=39255\" style=\"color: #98b7d2;\">BTE5476 - Project-Oriented Digital Signal Processing </a></div>\n",
    "</div>\n",
    "<div style=\"clear: both; font-size: 30pt; font-weight: bold; color: #64788b; margin-left: 30px;\">\n",
    "    Time and pitch scaling for audio\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.signal as sp\n",
    "from IPython.display import Audio\n",
    "from scipy.io import wavfile\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 14, 4 \n",
    "plt.rcParams['image.cmap'] = 'tab10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def multiplay(clips, sf, title=None):\n",
    "    outs = [widgets.Output() for c in clips]\n",
    "    for ix, item in enumerate(clips):\n",
    "        with outs[ix]:\n",
    "            print(title[ix] if title is not None else \"\")\n",
    "            display(Audio(item, rate=sf, normalize=True))\n",
    "    return widgets.HBox(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_spec(x, Fs, max_freq=None, do_fft=True):\n",
    "    C = int(len(x) / 2)  # positive frequencies only\n",
    "    if max_freq:\n",
    "        C = int(C * max_freq / float(Fs) * 2) \n",
    "    X = np.abs(np.fft.fft(x)[0:C]) if do_fft else x[0:C]\n",
    "    N = Fs * np.arange(0, C) / len(x);\n",
    "    plt.plot(N, X)\n",
    "    return N, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# pre-load audio samples in a dictionary\n",
    "audio, audio_sf = {}, None\n",
    "for file in ['speech', 'music', 'cymbal', 'clarinet', 'yesterday'] :\n",
    "    sf, x = wavfile.read(f'data/{file}.wav')\n",
    "    if audio_sf is None:\n",
    "        audio_sf = sf\n",
    "    else:\n",
    "        assert sf == audio_sf, 'notebook requires sampling rates for all audio samples to be the same'\n",
    "    audio[file] = (x - np.mean(x)) / 32767.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem setup\n",
    "\n",
    "Consider a finite-support analog audio signal $x_i(t)$ starting at $t=0$ and ending at $t=T$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Time, pitch, timbre\n",
    "\n",
    " * the duration of an audio signal is equal to the length $T$ of its finite support\n",
    " * a local periodicity, $x(t) \\approx x(t + nP)$, produces a sensation of _pitch_ with fundamental frequency $f_1 = 1/P$\n",
    " * the magnitude spectrum $|X(f)|$ determines the perceived _timbre_\n",
    "\n",
    "Note that the spectrum of a pitched signal has a _harmonic_ structure, that is, the energy is concentrated around multiples of the fundamental frequency $f_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plot_spec(audio['clarinet'], audio_sf);\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_spec(audio['cymbal'], audio_sf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The goal\n",
    "\n",
    " * time scaling: change the duration of a signal\n",
    " * pitch scaling: change the pitch of a signal\n",
    "\n",
    "Can we scale time and pitch independently of each other, and without affecting timbre?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Time scaling via warping\n",
    "\n",
    "The simplest form of time scaling is time warping, that is, we change the underlying timebase.\n",
    "\n",
    "## Continuous time\n",
    "<img width=\"300\" style=\"float: right;\" src=\"img/turntable.jpg\">\n",
    "\n",
    "For analog signals, warping the time axis is simply:\n",
    "$$\n",
    "    x_o(t) = x_i(\\alpha t), \\qquad \\alpha > 0\n",
    "$$\n",
    "\n",
    " * $\\alpha > 1$ : signal plays faster\n",
    " * $\\alpha < 1$ : signal plays slower\n",
    "\n",
    "This is what happens if you spin a vynil record faster or slower than its nominal RPM value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discrete time\n",
    "\n",
    "For $\\alpha = N/M$ with $N, M$ coprime we can use multirate processing:\n",
    "\n",
    "<img width=\"600\" style=\"float:right;\" src=\"img/multirate.png\">\n",
    "\n",
    " * upsample by $N$\n",
    " * apply a lowpass filter\n",
    " * downsample by $M$\n",
    " \n",
    "but this can get very expensive computationally if $N$ is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fractional resampling\n",
    "\n",
    "For efficiency, we can approximate the exact multirate approach via local interpolation. For instance, using _linear_ interpolation, each output sample $x_o[m]$ is computed like so:\n",
    "\n",
    " * find the two closest input samples $x_i[n]$ and $x_i[n+1]$ where $n = \\lfloor \\alpha m \\rfloor$\n",
    " * $\\tau = \\alpha m - n$ is the ``time'' offset between output and input ($0 \\le \\tau \\le 1$)\n",
    " * compute the linear interpolation between $x[n]$ and $x[n+1]$ at $t=\\tau$: $$\n",
    "   x_o[m] = (1-\\tau)x_i[n] + \\tau\\, x_i[n+1]\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def resample(x, alpha):\n",
    "    N = len(x)\n",
    "    y, m = np.zeros(int(N / alpha + 0.5)), 0\n",
    "    while True:\n",
    "        t = m * alpha \n",
    "        n = int(np.floor(t))\n",
    "        if n < N - 1:\n",
    "            y[m] = (n + 1 - t) * x[n] + (t - n) * x[n + 1] \n",
    "            m += 1\n",
    "        else:\n",
    "            break\n",
    "    return y[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "factors = [1.4, 0.8]\n",
    "for file in ['speech', 'music']:\n",
    "    display(multiplay(\n",
    "        [audio[file], *(resample(audio[file], a) for a in factors)], \n",
    "        audio_sf, \n",
    "        [f'{file} sample', *('slower' if a < 1 else 'faster' for a in factors)] \n",
    "    ))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Issues with time warping:\n",
    "\n",
    "If the time axis is scaled by a factor $\\alpha$:\n",
    "\n",
    " * period is scaled by $1/\\alpha$ since $x_i(t) = x_i(t + nP) \\Rightarrow x_o(t) \\approx x_o(t + nP/\\alpha)$; \\\n",
    "   pitch is scaled as $f_o = \\alpha f_i$ and we hear a higher pitch if $\\alpha > 1$, lower if $\\alpha < 1$\n",
    " * timbre is affected because $X_o(f) = X(f / \\alpha)$; \\\n",
    "   if $\\alpha > 1$ the spectrum is stretched and everything sounds shrill (chipmunk voice), if $\\alpha < 1$ the spectrum shrinks around zero and everything sounds dark (Darth Vader voice)\n",
    " * the method cannot be implemented in real time (it cannot be driven by the input signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Frequency shifting\n",
    "\n",
    "To change the pitch of an audio signal we could try to simply shift its spectrum up or down in frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shifting via modulation\n",
    "\n",
    "Using the modulation theorem: \n",
    "$$\n",
    "\\mathrm{DTFT}\\{2x[n]\\cos(\\omega_0 n)\\} = X(\\omega - \\omega_0) + X(\\omega + \\omega_0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modshift(x, f, sf):\n",
    "    return 2 * x * np.cos(2 * np.pi / audio_sf * f * np.arange(0, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = audio['speech']\n",
    "shift = 400\n",
    "\n",
    "x_mod = modshift(x, shift, audio_sf)\n",
    "multiplay([x, x_mod], audio_sf, ['original', 'upshift via modulation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img width=\"250\" style=\"float: right; margin: 10px;\" src=\"img/voice_changer.jpg\">\n",
    "\n",
    "Simple technique to make your voice hard to recognize, low-cost and real-time; used in old-time \"voice scramblers\" for anonymous prank calls. However:\n",
    "\n",
    " * audio is baseband, so we can only go \"up\"\n",
    " * signal's bandwidth much larger than shift frequency: the two spectral copies overlap (distortion)\n",
    " * inaudible low frequencies are shifted to hearing range (warbling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    " * negative frequencies are aliased to positive\n",
    " * lots of spurious content in low frequency region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plot_spec(x, audio_sf, 2000)\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_spec(x_mod, audio_sf, 2000)\n",
    "plt.axvline(shift, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improved modulation-based shifting\n",
    "\n",
    "Use the same approach as in SSB (single-sideband modulation):\n",
    "\n",
    "<img width=\"600\" style=\"\" src=\"img/ssb.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Algorithm:\n",
    " * apply a bandpass filter to eliminate inaudible low frequencies and to limit bandwidth\n",
    " * if shifting down, also eliminate low frequencies below modulation frequency\n",
    " * compute the one-sided analytic signal using a Hilbert filter\n",
    " * shift analytic signal via a complex exponential\n",
    " * take the real part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ssbshift(x, f, sf, L=40):\n",
    "    # bandpass the voice signal\n",
    "    f0 = 100.0 if f > 0 else 100.0 - f\n",
    "    x = sp.lfilter(*sp.butter(6, [f0, min(10000, 0.9 * sf/2)], fs=sf, btype='band'), x)\n",
    "    # compute analytic signal\n",
    "    h = sp.remez(2 * L + 1, [0.02, 0.48], [-1], type='hilbert')\n",
    "    xa = 1j * sp.lfilter(h, 1, x)[L:] + x[:-L]\n",
    "    # shift and take real part\n",
    "    return np.real(xa * np.exp(1j * 2 * np.pi * f / sf * np.arange(0, len(xa))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ssb = ssbshift(x, shift, audio_sf)\n",
    "\n",
    "multiplay([x, x_ssb, ssbshift(x, -shift, audio_sf)], audio_sf, ['original', 'ssb upshift', 'ssb downshift'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare simple modulation and SSB modulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplay([x, x_mod, x_ssb], audio_sf, ['original', 'simple modulation', 'ssb modulation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plot_spec(x, audio_sf, 2000)\n",
    "plt.subplot(3, 1, 2)\n",
    "plot_spec(x_mod, audio_sf, 2000)\n",
    "plt.axvline(shift, color='red')\n",
    "plt.subplot(3, 1, 3)\n",
    "plot_spec(x_ssb, audio_sf, 2000)\n",
    "plt.axvline(shift, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## _The_ issue with frequency shifting\n",
    "\n",
    "Good things so far: \n",
    " * can be implemented in real time\n",
    " * preserves the speed of the original audio\n",
    " \n",
    "But it sounds **awful** with music:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = audio['music']\n",
    "shift=200\n",
    "multiplay([x, ssbshift(x, shift, audio_sf), ssbshift(x, -shift, audio_sf)], \n",
    "          audio_sf, \n",
    "          ['music sample', 'ssb upshift', 'ssb downshift'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In pitch perception, the brain checks for a _harmonic_ spectral structure, with spectral lines at multiples of a fundamental frequency:\n",
    "\n",
    "$$\n",
    "    f_n = nf_1, \\quad n = 1, 2, 3, \\ldots \\Rightarrow \\frac{f_n}{f_1} = n\n",
    "$$\n",
    "\n",
    "Spectral shifting breaks the harmonic structure:\n",
    "\n",
    "$$\n",
    "    f'_n = f_c + f_n = f_c + nf_1 \\Rightarrow \\frac{f'_n}{f'_1} = \\frac{f_c + nf_1}{f_c + f_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Granular Synthesis\n",
    "<img width=\"500\" style=\"float: right;\" src=\"img/gsplot.jpg\">\n",
    "\n",
    "In [Granular Synthesis](https://en.wikipedia.org/wiki/Granular_synthesis) complex waveforms are be built by stitching together very short sound snippets called \"grains\". \n",
    " \n",
    " * used as a compositional tool to generate complex timbres at arbitrary pitches\n",
    " * each grain must be \"pitched\"\n",
    " * works well for non-pitched sounds too\n",
    " * lots of sophisticated variations exist to maximize output quality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "By decomposing a signal into grains and then putting the grains back together we can try to change the overall duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time stretching via granular synthesis \n",
    "\n",
    "Simple idea to increase the duration of a signal: \n",
    " * split signal into small grains\n",
    " * repeat each grain two or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_poc(x, grain_size, M=2):\n",
    "    y = np.zeros(M * (len(x) + 1))\n",
    "    for n in range(0, len(x) - grain_size, grain_size):\n",
    "        y[M * n : M * (n + grain_size)] = np.tile(x[n : n + grain_size], M)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Helper function to convert milliseconds to samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms2n(ms, sf):\n",
    "    return int(float(sf) * ms / 1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain_size = ms2n(40, audio_sf)\n",
    "\n",
    "for file in ['speech', 'music']:\n",
    "    display(multiplay(\n",
    "        [audio[file], gs_poc(audio[file], grain_size)], audio_sf, \n",
    "        [f'{file} sample', 'stretched'] \n",
    "    ))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good:\n",
    " * pitch and timbre are well-preserved\n",
    " * very cheap to implement\n",
    "\n",
    "The bad:\n",
    " * grain repetition creates discontinuities in the signal that we hear as a clicking noise \n",
    " * can we use granular synthesis also to _reduce_ the length of a signal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Crossfading grains\n",
    "\n",
    "In granular synthesis, grains should overlap and be _merged_ together using a multiplicative *tapering window*. If $g_k[n]$ is a set of grains of length $N$, the output signal is defined as \n",
    "\n",
    "$$\n",
    "    x_o[n] = \\sum_{k=-\\infty}^\\infty g_k[n-kS]w[n - kS]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    " * $w[n]$ is a finite-support window of length $N$, smoothly tapering to zero at both ends\n",
    " * $S$ is the synthesis _stride_ (that is $S = N(1-a)$ where $0 \\le a \\le 1$ is the amount of overlap between successive grains)\n",
    " * the window should fulfill the COLA condition (constant overlap-add):$$\n",
    "   \\sum_{k=-\\infty}^\\infty w[n - kS] = 1\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### COLA windows\n",
    "\n",
    "There is a vast literature on the design of windows with the COLA property; here, for simplicity, we will use the Hann (or Hanning) window, defined as \n",
    "\n",
    "$$\n",
    "    w[n] = \\frac{1}{2}\\left[1-\\cos\\left(\\frac{2\\pi n}{N-1}\\right)\\right], \\quad n = 0, 1, \\ldots, N-1.\n",
    "$$\n",
    "\n",
    "The Hann window fulfills the constant overlap-add condition when the stride is of the form \n",
    "$$\n",
    "    S = \\frac{N}{K}\n",
    "$$\n",
    "where $K$ is an integer greater than one; since the stride must also be an integer, depending on the value of $K$ we may need to adjust the window length. Note that the overlap factor is $a = (K-1)/K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hann_cola(N, K=2):\n",
    "    assert K >= 2, 'cannot satisfy COLA with zero overlap. Set K > 1'\n",
    "    S = N // K\n",
    "    N = K * S + 1\n",
    "    win = (2 / K) * np.sin(np.pi * np.arange(0, N) / (N-1)) ** 2\n",
    "    return win, N, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def test_overlap(win, N, S, M=8):\n",
    "    assert N == len(win)\n",
    "    y = np.zeros((M - 1) * S + N)\n",
    "    for m in range(0, M * S, S):\n",
    "        plt.plot(m + np.arange(0, N), win, 'C0')\n",
    "        y[m:m+N] += win\n",
    "    plt.plot(y, 'C2', linewidth=8, alpha=0.3)\n",
    "    plt.gca().set_ylim([0, 1.2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_overlap(*hann_cola(50, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_overlap(*hann_cola(101, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time scaling with granular synthesis\n",
    "\n",
    "<img width=\"800\" style=\"float: right;\" src=\"img/ola.png\">\n",
    "\n",
    "For a scaling factor $\\alpha$: \n",
    "\n",
    " * choose grain size (around 40 ms for speech, 100 ms for music) \n",
    " * choose overlap factor and obtain grain stride $S$ (in samples)\n",
    " * iterate over $k = 0, 1, \\ldots$:\n",
    "     * extract a grain from the input at sample $k \\lfloor S \\alpha \\rfloor$  \n",
    "     * blend the grain into the output signal at sample $kS$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def timescale_gs(x, alpha, grain_size, K=2):\n",
    "    win, N, S = hann_cola(grain_size, K)\n",
    "    y = np.zeros(int(len(x) / alpha))\n",
    "    n, m = 0, 0\n",
    "    while n < len(x) - N and m < len(y) - N:\n",
    "        y[m:m+N] += x[n:n+N] * win\n",
    "        m += S\n",
    "        n += int(S * alpha)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "factors = [1.4, 0.8]\n",
    "\n",
    "for file, gs_ms in [('speech', 40), ('music', 100)]:\n",
    "    gs = ms2n(gs_ms, audio_sf)\n",
    "    display(multiplay(\n",
    "        [audio[file], *(timescale_gs(audio[file], a, gs, K=2) for a in factors)], \n",
    "        audio_sf, \n",
    "        [f'{file} sample', *('slower' if a < 1 else 'faster' for a in factors)] \n",
    "    ))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remarks so far:\n",
    "  * time stretching works best to speed up speech\n",
    "  * slowed down speech still has clicking artefacts\n",
    "  * music is OK but the lower pitches have significant detuning\n",
    "  \n",
    "There seems to be a problem with low frequencies and still significant artefacts. We'll talk more about those later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pitch shifting via granular synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since GS allows us to change the time scale leaving the pitch unchanged, what if:\n",
    " * we use GS to scale time by a factor $1/\\alpha$; pitch remains the same\n",
    " * we apply fractional resampling (i.e. time warping) by a factor $\\alpha$; this restores the original length but changes the pitch as a side effect.\n",
    " * pitch is raised if $\\alpha > 1$, lowered otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following implementation uses two passes over the input, the first to time-scale it with granular synthesis, the second to resample it; this approach works but it cannot be used in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitchshift_gs(x, alpha, grain_size, K=2):\n",
    "    return(resample(timescale_gs(x, 1 / alpha, grain_size, K), alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semitone = 2 ** (1.0 / 12)\n",
    "\n",
    "x = audio['music']\n",
    "grain_size = ms2n(100, audio_sf)\n",
    "\n",
    "shift = 3\n",
    "alpha = semitone ** shift\n",
    "multiplay([x, pitchshift_gs(x, alpha, grain_size), pitchshift_gs(x, 1 / alpha, grain_size)], \n",
    "          audio_sf, \n",
    "          title=['music sample', f'up {shift} semitones (GS)', f'down {shift} semitones (GS)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise: real-time implementation\n",
    "\n",
    "We want to implement a pitch shifting algorithm that works in real time, namely:\n",
    "\n",
    " * the algorithm produces one output sample every new input sample, synchronously\n",
    " * we can accept a reasonable amount of fixed processing delay\n",
    " * the algorithm can only access past input samples\n",
    "\n",
    "The fundamental idea is to use granular synthesis to merge *resmapled* grains directly; let's break the problem into manageable pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time dummy granular synthesis\n",
    "\n",
    "Consider a granular synthesis function that simply reconstruct the input signal; from the general formula\n",
    "\n",
    "$$\n",
    "    x_o[n] = \\sum_{k=-\\infty}^\\infty g_k[n-kS]w[n - kS]\n",
    "$$\n",
    "\n",
    "if we define each grain as $g_k[n] = x_i[kS+n]$, we can see that the output is equal to the input as long as the COLA condition is satisfied:\n",
    "\n",
    "$$\n",
    "    x_o[n] = \\sum_{k=-\\infty}^\\infty x_i[n]w[n - kS] = x_i[n] \\sum_{k=-\\infty}^\\infty w[n - kS] = x_i[n]\n",
    "$$ \n",
    "\n",
    "Assume the window has length $N$, with overlap factor equal to $K$ and a stride of $S$ samples; since the window is nonzero only over the $[0, N-1]$ interval, every output sample is the sum of weighed samples from $K$ grains since\n",
    "$$\n",
    "    w[n-kS] \\neq 0 \\quad \\Leftrightarrow \\quad 0 \\le n - kS < N \\quad \\Leftrightarrow \\quad n/S - K < k \\le n/S\n",
    "$$\n",
    "\n",
    "If we store incoming input samples $x_i[n]$ in a memory buffer\n",
    " * when $n = kS$ (at multiples of the stride) the buffer will have accumulated the data for grain $g_{k-1}(m)$\n",
    " * at time $n$ the buffer will contain data for grains $g_{k-1}(m), g_{k-2}(m), \\ldots$, where $k = \\lfloor n / S \\rfloor$\n",
    " * at time $n$ we will be able to output $x_o[n - N + 1]$\n",
    " * the total delay is therefore $N$ samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following picture illustrates the buffering structure, where solid lines indicate full grains already in the buffer and dashed lines grains not yet acquired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win, N, S = hann_cola(100, K=3)\n",
    "D = int(S * 0.8)\n",
    "for g in [(-2 + i, 'C'+c, f'$g_{{k{-3+i:+}}}$') for i, c in enumerate(('0', '9', '2', '6:', '3:', '1:'))]:\n",
    "    plt.plot(np.arange(N) + g[0] * S, win, g[1], label=g[2])\n",
    "plt.axvline(N+D, c='C2', lw=3)\n",
    "plt.axvline(D+1, c='C2', ls='--')\n",
    "plt.gca().set_xticks([D+1, N-S-1, N-1, N+D], ['$n-N+1$', '$(k-1)S$', '$kS$', '$n$'])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_gs(x, grain_size, K=2):\n",
    "    win, N, S = hann_cola(grain_size, K)\n",
    "\n",
    "    # keep in memory the last K grains \n",
    "    grains = np.zeros((K, N))\n",
    "\n",
    "    # input buffer, you can make this as large as you want (at least as big as a grain)\n",
    "    B = 2 * N\n",
    "    buffer = np.zeros(B)\n",
    "\n",
    "    y = np.zeros(len(x))\n",
    "    for n in range(len(x)):\n",
    "        # your code here\n",
    "        pass\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the function on a ramp; output should be a delayed version of the input\n",
    "x = np.arange(1, 100)\n",
    "plt.plot(x)\n",
    "plt.plot(dummy_gs(x, 10, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time pitch shifting granular synthesis\n",
    "\n",
    "With the \"dummy\" granular synthesis in place, the trick now is to blend together time-warped grains. For this we will need to do the following\n",
    "\n",
    " * for a given grain size $N$, we will need to resample a chunk of $M \\approx \\alpha N$ samples, where $\\alpha$ is the resampling factor\n",
    " * we need to allocate an input buffer of size at least $M$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pitchshift_gs_rt(x, alpha, grain_size, K=2):\n",
    "    y = np.zeros(len(x))\n",
    "    # your code here\n",
    "    \n",
    "    for n in range(len(x)):\n",
    "        # your code here\n",
    "        y[n] = ...\n",
    "        pass\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = audio['music']\n",
    "grain_size = ms2n(100, audio_sf)\n",
    "\n",
    "shift = 3\n",
    "alpha = semitone ** shift\n",
    "multiplay([x, pitchshift_gs_rt(x, alpha, grain_size), pitchshift_gs_rt(x, 1 / alpha, grain_size)], \n",
    "          audio_sf, \n",
    "          title=['music sample', f'up {shift} semitones (GS)', f'down {shift} semitones (GS)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Before DSP...\n",
    "\n",
    "<img width=\"600\" style=\"\" src=\"img/pitchshift.jpg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Although we have just described a purely digital version of grain-based pitch shifting, it is interesting to remark that, before digital audio was a reality, the only true pitch-shifting devices available to the music industry were extremely complex (and costly) mechanical devices that implemented, in analog, the same principle behind granular synthesis. \n",
    "\n",
    "Above is the block diagram of such a contraption: the original sound is recorded on the main tape spool, which is run at a speed that can vary with respect to the nominal recording speed to raise or lower the pitch. To compensate for these changes in speed the tape head is actually a rotating disk containing four individual coils; at any given time, at least two neighboring coils are picking up the signal from the tape, with an automatic fade-in and fade-out as they approach and leave the tape. The head disk rotates at a speed that compensates for the change in speed of the main tape, therefore keeping the timebase constant. The coils on the head disk picking up the signal are in fact producing overlapping \"grains\" that are mixed together in the output signal.  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
